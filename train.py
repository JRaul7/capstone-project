# -*- coding: utf-8 -*-
"""train_capstone.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nmI-s0iJ3IsVsWvi7pjZ3mlhD1F-16-S
"""

import os
import shutil
import numpy as np
import pandas  as pd
import matplotlib
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow.lite as tflite

from tensorflow import keras
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import load_model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.layers import Flatten
from tensorflow.keras.optimizers.legacy import Adam

from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from tensorflow.keras.layers import BatchNormalization

#Parameters for the model

learning_rate = 0.001
epochs = 6
optimizer = Adam(learning_rate=learning_rate, clipnorm=0.9)

# load train and test dataset
def load_dataset():

  letter_dict = {
    0: 10, 1: 11, 2: 12, 3: 13, 4: 14, 5: 15, 6: 16, 7: 17, 8: 18, 9: 19, 10: 20,
    11: 21, 12: 22, 13: 23, 14: 24, 15: 25, 16: 26, 17: 27, 18: 28, 19: 29, 20: 30,
    21: 31, 22: 32, 23: 33, 24: 34, 25: 35
   }
  letter_full_data = pd.read_csv('./letters_handwritten_data.csv')
  letter_full_data.rename(columns={'0':'label'}, inplace=True)
  letter_full_data['label']= letter_full_data['label'].map(letter_dict)

  x_letter = letter_full_data.drop('label', axis = 1)
  x_letter = np.reshape(x_letter.values, (x_letter.shape[0], 28, 28))
  y_letter = letter_full_data['label']
  (trainX, trainY), (testX, testY) = mnist.load_data()

  x_number = np.vstack([trainX, testX])
  y_number = np.hstack([trainY, testY])
  x_data = np.vstack([x_letter, x_number])
  y_data = np.hstack([y_letter, y_number])
  (trainX, testX, trainY, testY) = train_test_split(x_data,
	y_data, test_size=0.20, stratify=y_data, random_state=42)

	# reshape dataset to have a single channel
  trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))
  testX = testX.reshape((testX.shape[0], 28, 28, 1))
	# one hot encode target values
  trainY = to_categorical(trainY, num_classes = 36, dtype = 'int')
  testY = to_categorical(testY, num_classes = 36, dtype = 'int')

  return trainX, trainY, testX, testY

# define cnn model
def define_model(optimizer):
	model = Sequential()
	model.add(Conv2D(32, (5, 5), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))
	model.add(BatchNormalization()) #Used to avoid 'gradient explosion' as seen in previous experiments with this dataset
	model.add(MaxPooling2D((2, 2)))
	model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))
	model.add(Dropout(0.2)) #Used to avoid 'gradient explosion' as seen in previous experiments with this dataset
	model.add(Flatten())
	model.add(Dense(36, activation='softmax')) #36 is the total number of labels in the dataset
	opt = optimizer
	model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
	return model

# evaluate the deep model on the test dataset

def prep_pixels(train, test):
	# convert from integers to floats
	train_norm = train.astype('float32')
	test_norm = test.astype('float32')
	# normalize to range 0-1
	train_norm = train_norm / 255.0
	test_norm = test_norm / 255.0
	# return normalized images
	return train_norm, test_norm

# evaluating a model
def validation_model(epochs):
	print('the model is being evaluated on the test dataset')
	# load dataset
	trainX, trainY, testX, testY = load_dataset()
	# prepare pixel data
	trainX, testX = prep_pixels(trainX, testX)
  # define model
	model = define_model(optimizer)
  # fit model
	model.fit(trainX, trainY, epochs=epochs, batch_size=32, validation_data = (testX, testY))
	# evaluate model on test dataset
	_, acc = model.evaluate(testX, testY, verbose=0)
	print(f'Model with learning rate: {learning_rate} the accuracy is:')
	print('> %.3f' % (acc * 100.0))
	# save model
	print(f'Saving Initial Model with learning rate: {learning_rate}:')
	model.save('initial_model.h5')

validation_model(epochs)

# training the final model

print('training the final model')

def run_final_model(epochs):
  trainX, trainY, testX, testY = load_dataset()
  trainX, testX = prep_pixels(trainX, testX)
  x_dataset = np.vstack([trainX, testX])
  y_dataset = np.vstack([trainY, testY])
  model = define_model(optimizer)
  # fit model
  model.fit(trainX, trainY, epochs=epochs, batch_size=32)
  # save model
  print(f'Saving final CNN Model with learning rate: {learning_rate}:')
  model.save('cnn_model.h5')
  model.save_weights('cnn_model.weights.h5', overwrite=True)
  print('the final model has been saved')
  # Convert the Keras model to TensorFlow Lite model
  converter = tf.lite.TFLiteConverter.from_keras_model(model)
  tflite_model = converter.convert()
  # Save the TensorFlow Lite model
  with open('cnn_model.tflite', 'wb') as f:
    f.write(tflite_model)


# run final

run_final_model(epochs)